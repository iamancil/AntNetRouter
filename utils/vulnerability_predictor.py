"""
Vulnerability Prediction Module
This module predicts potential vulnerabilities in IoT networks.
"""

import logging
import time
import math
from typing import Dict, List, Tuple, Optional, Any
from collections import defaultdict, Counter

# Set up logging
logger = logging.getLogger(__name__)

class VulnerabilityPredictor:
    """
    Predicts potential vulnerabilities in IoT network based on topology,
    security events, and device characteristics.
    """
    
    def __init__(self):
        """
        Initialize the Vulnerability Predictor
        """
        # Risk weights for different factors
        self.risk_weights = {
            'security_score': 0.3,      # Weight for node security score
            'connection_count': 0.15,   # Weight for number of connections
            'security_events': 0.25,    # Weight for recent security events
            'protocol_risk': 0.2,       # Weight for protocol risks
            'node_type': 0.1           # Weight for node type (some are riskier)
        }
        
        # Node type risk factors
        self.node_type_risks = {
            'router': 0.8,              # High risk due to central role
            'gateway': 0.9,             # Very high risk as entry point
            'server': 0.7,              # High risk due to services
            'camera': 0.6,              # Medium-high risk due to common vulnerabilities
            'sensor': 0.4,              # Medium risk
            'actuator': 0.5,            # Medium risk
            'mobile': 0.6,              # Medium-high risk due to mobility
            'embedded': 0.5,            # Medium risk
            'unknown': 0.7              # High risk due to uncertainty
        }
        
        # Protocol risk factors
        self.protocol_risks = {
            'HTTP': 0.7,                # Unencrypted HTTP
            'HTTPS': 0.4,               # Encrypted but still has vulnerabilities
            'FTP': 0.8,                 # Unencrypted file transfer
            'TELNET': 0.9,              # Very high risk (unencrypted)
            'SSH': 0.3,                 # Encrypted but can be misconfigured
            'MQTT': 0.6,                # IoT protocol with security concerns
            'COAP': 0.6,                # IoT protocol with security concerns
            'DNS': 0.5,                 # Medium risk
            'DHCP': 0.5,                # Medium risk
            'Unknown': 0.7              # High risk due to uncertainty
        }
        
        # Common vulnerability patterns
        self.vulnerability_patterns = {
            'weak_auth': {
                'description': 'Weak authentication mechanisms',
                'indicators': ['failed login attempts', 'default credentials'],
                'applies_to': ['router', 'gateway', 'camera', 'server'],
                'cvss_range': (7.0, 9.0),
                'recommended_actions': [
                    'Implement strong password policies',
                    'Enable two-factor authentication',
                    'Disable default credentials'
                ]
            },
            'unencrypted_comm': {
                'description': 'Unencrypted communications',
                'indicators': ['HTTP traffic', 'FTP traffic', 'TELNET traffic'],
                'applies_to': ['all'],
                'cvss_range': (5.0, 7.0),
                'recommended_actions': [
                    'Implement TLS/SSL encryption',
                    'Update to secure protocols',
                    'Implement secure communication policies'
                ]
            },
            'outdated_firmware': {
                'description': 'Outdated firmware or software',
                'indicators': ['long uptime', 'known vulnerabilities', 'old protocol versions'],
                'applies_to': ['router', 'gateway', 'camera', 'sensor', 'actuator'],
                'cvss_range': (6.0, 9.0),
                'recommended_actions': [
                    'Implement regular firmware updates',
                    'Set up automatic updates',
                    'Create firmware update policy'
                ]
            },
            'insecure_ports': {
                'description': 'Unnecessarily exposed services/ports',
                'indicators': ['multiple open ports', 'unusual port access'],
                'applies_to': ['server', 'router', 'gateway'],
                'cvss_range': (4.0, 7.0),
                'recommended_actions': [
                    'Implement proper firewall rules',
                    'Close unnecessary ports',
                    'Use port knocking or VPN for admin access'
                ]
            },
            'privilege_escalation': {
                'description': 'Potential for privilege escalation',
                'indicators': ['unusual access patterns', 'multiple failed admin logins'],
                'applies_to': ['server', 'router', 'gateway'],
                'cvss_range': (7.0, 9.0),
                'recommended_actions': [
                    'Implement principle of least privilege',
                    'Regular security audits',
                    'Application sandboxing'
                ]
            },
            'data_exposure': {
                'description': 'Sensitive data exposure',
                'indicators': ['large data transfers', 'unencrypted storage'],
                'applies_to': ['server', 'camera', 'sensor'],
                'cvss_range': (5.0, 8.0),
                'recommended_actions': [
                    'Implement data encryption at rest',
                    'Data Loss Prevention (DLP) solutions',
                    'Regular data audits'
                ]
            },
            'iot_vulnerable': {
                'description': 'IoT-specific vulnerabilities',
                'indicators': ['IoT protocols', 'minimal resources', 'embedded systems'],
                'applies_to': ['sensor', 'actuator', 'camera', 'embedded'],
                'cvss_range': (6.0, 8.0),
                'recommended_actions': [
                    'IoT-specific security framework',
                    'Segmentation of IoT networks',
                    'Regular IoT security assessments'
                ]
            },
            'ddos_target': {
                'description': 'Potential DDoS amplification target',
                'indicators': ['public-facing services', 'high bandwidth', 'UDP services'],
                'applies_to': ['server', 'router', 'gateway'],
                'cvss_range': (4.0, 7.0),
                'recommended_actions': [
                    'DDoS protection services',
                    'Rate limiting',
                    'Traffic filtering'
                ]
            },
            'compromise_pivot': {
                'description': 'Potential pivot point for compromise',
                'indicators': ['central position in network', 'high connectivity', 'access to multiple segments'],
                'applies_to': ['router', 'gateway', 'server'],
                'cvss_range': (7.0, 9.0),
                'recommended_actions': [
                    'Network segmentation',
                    'Zero trust architecture',
                    'Enhanced monitoring for pivotal systems'
                ]
            }
        }
        
        # Risk prediction time horizons
        self.time_horizons = [
            {'name': 'short_term', 'days': 1, 'weight': 0.5},
            {'name': 'medium_term', 'days': 7, 'weight': 0.3},
            {'name': 'long_term', 'days': 30, 'weight': 0.2}
        ]
        
        logger.info("Vulnerability Predictor initialized")
    
    def predict_node_vulnerabilities(self, node_id: int, node_data: Dict, 
                                   security_events: List[Dict], network_stats: Dict) -> Dict:
        """
        Predict vulnerabilities for a specific node
        
        Args:
            node_id: ID of the node to analyze
            node_data: Data about the node
            security_events: Recent security events related to the node
            network_stats: Overall network statistics
            
        Returns:
            Dictionary with vulnerability predictions
        """
        # Calculate base risk score
        base_risk = self._calculate_base_risk(node_data)
        
        # Adjust for recent security events
        event_risk = self._calculate_event_risk(security_events)
        
        # Adjust for network position
        network_risk = self._calculate_network_position_risk(node_id, node_data, network_stats)
        
        # Identify potential vulnerabilities
        vulnerabilities = self._identify_potential_vulnerabilities(node_data, security_events)
        
        # Calculate overall risk score (0-1 scale)
        overall_risk = (base_risk * self.risk_weights['security_score'] + 
                       event_risk * self.risk_weights['security_events'] + 
                       network_risk * self.risk_weights['connection_count'])
        
        # Normalize to 0-1 scale
        overall_risk = min(max(overall_risk, 0.0), 1.0)
        
        # Determine risk level category
        risk_level = self._determine_risk_level(overall_risk)
        
        # Generate time-based predictions
        predictions = self._generate_time_predictions(node_data, overall_risk, vulnerabilities)
        
        # Build the prediction result
        result = {
            'node_id': node_id,
            'timestamp': time.time(),
            'overall_risk_score': overall_risk,
            'risk_level': risk_level,
            'vulnerabilities': vulnerabilities,
            'predictions': predictions,
            'contributing_factors': {
                'base_risk': base_risk,
                'event_risk': event_risk,
                'network_risk': network_risk
            },
            'recommended_actions': self._generate_recommendations(vulnerabilities)
        }
        
        return result
    
    def predict_network_vulnerabilities(self, network_data: Dict) -> Dict:
        """
        Predict vulnerabilities for the entire network
        
        Args:
            network_data: Data about the network
            
        Returns:
            Dictionary with network vulnerability predictions
        """
        nodes = network_data.get('nodes', {})
        security_events = network_data.get('security_events', [])
        
        # Calculate network-wide statistics
        network_stats = self._calculate_network_stats(nodes)
        
        # Identify most vulnerable nodes
        vulnerable_nodes = []
        for node_id, node_data in nodes.items():
            # Get events related to this node
            node_events = [event for event in security_events 
                         if event.get('node_id') == node_id]
            
            # Calculate vulnerability
            vulnerability = self.predict_node_vulnerabilities(
                node_id, node_data, node_events, network_stats
            )
            
            vulnerable_nodes.append({
                'node_id': node_id,
                'risk_score': vulnerability['overall_risk_score'],
                'risk_level': vulnerability['risk_level'],
                'predicted_vulnerabilities': [v['type'] for v in vulnerability['vulnerabilities']]
            })
        
        # Sort by risk score (highest first)
        vulnerable_nodes.sort(key=lambda x: x['risk_score'], reverse=True)
        
        # Identify critical paths and segments
        critical_paths = self._identify_critical_paths(nodes, vulnerable_nodes)
        
        # Identify attack vectors based on network topology
        attack_vectors = self._identify_attack_vectors(nodes, vulnerable_nodes, security_events)
        
        # Calculate network risk metrics
        avg_node_risk = sum(node['risk_score'] for node in vulnerable_nodes) / len(vulnerable_nodes) if vulnerable_nodes else 0
        high_risk_count = sum(1 for node in vulnerable_nodes if node['risk_level'] in ['High', 'Critical'])
        risk_distribution = Counter(node['risk_level'] for node in vulnerable_nodes)
        
        # Generate network-wide predictions
        predictions = self._generate_network_predictions(
            avg_node_risk, high_risk_count, len(nodes), critical_paths, attack_vectors
        )
        
        # Build the prediction result
        result = {
            'timestamp': time.time(),
            'overall_network_risk': avg_node_risk,
            'network_risk_level': self._determine_risk_level(avg_node_risk),
            'vulnerable_nodes': vulnerable_nodes[:10],  # Top 10 most vulnerable
            'critical_paths': critical_paths,
            'attack_vectors': attack_vectors,
            'statistics': {
                'total_nodes': len(nodes),
                'high_risk_nodes': high_risk_count,
                'avg_node_risk': avg_node_risk,
                'risk_distribution': risk_distribution
            },
            'predictions': predictions,
            'recommended_actions': self._generate_network_recommendations(
                vulnerable_nodes, critical_paths, attack_vectors
            )
        }
        
        return result
    
    def predict_vulnerability_propagation(self, network_data: Dict, vulnerable_node_id: int) -> Dict:
        """
        Predict how a vulnerability in one node might propagate through the network
        
        Args:
            network_data: Data about the network
            vulnerable_node_id: ID of the node with a vulnerability
            
        Returns:
            Dictionary with vulnerability propagation prediction
        """
        nodes = network_data.get('nodes', {})
        
        # Check if vulnerable node exists
        if str(vulnerable_node_id) not in nodes and vulnerable_node_id not in nodes:
            return {
                'error': f"Node with ID {vulnerable_node_id} not found",
                'propagation_paths': [],
                'affected_nodes': []
            }
        
        # Get node connections
        connections = self._get_node_connections(network_data, vulnerable_node_id)
        
        # Calculate propagation probabilities
        propagation_results = self._calculate_propagation(network_data, vulnerable_node_id, connections)
        
        # Build propagation paths
        propagation_paths = propagation_results['paths']
        
        # Calculate affected nodes
        affected_nodes = propagation_results['affected_nodes']
        
        # Generate remediation strategies
        remediations = self._generate_propagation_remediations(
            network_data, vulnerable_node_id, propagation_paths
        )
        
        # Build the prediction result
        result = {
            'vulnerable_node': vulnerable_node_id,
            'timestamp': time.time(),
            'propagation_paths': propagation_paths,
            'affected_nodes': affected_nodes,
            'propagation_probability': propagation_results['probability'],
            'worst_case_impact': propagation_results['worst_case_impact'],
            'time_to_propagate': propagation_results['time_to_propagate'],
            'recommended_remediations': remediations
        }
        
        return result
    
    def generate_remediation_plan(self, predictions: Dict) -> Dict:
        """
        Generate a remediation plan based on vulnerability predictions
        
        Args:
            predictions: Vulnerability predictions
            
        Returns:
            Dictionary with remediation plan
        """
        # Extract vulnerabilities
        if 'vulnerabilities' in predictions:
            # Node-level predictions
            vulnerabilities = predictions['vulnerabilities']
            node_id = predictions.get('node_id')
            plan_type = 'node'
        elif 'vulnerable_nodes' in predictions:
            # Network-level predictions
            vulnerabilities = []
            for node in predictions['vulnerable_nodes']:
                for vuln_type in node.get('predicted_vulnerabilities', []):
                    vulnerabilities.append({
                        'type': vuln_type,
                        'node_id': node['node_id']
                    })
            plan_type = 'network'
        else:
            return {'error': 'Invalid prediction data format'}
        
        # Generate remediation steps
        remediation_steps = []
        
        if plan_type == 'node':
            # Node-specific remediation
            for vulnerability in vulnerabilities:
                vuln_type = vulnerability['type']
                pattern = self.vulnerability_patterns.get(vuln_type, {})
                
                remediation_steps.append({
                    'vulnerability': vuln_type,
                    'description': pattern.get('description', 'Unknown vulnerability'),
                    'severity': self._calculate_severity(vulnerability),
                    'steps': pattern.get('recommended_actions', []),
                    'timeline': self._suggest_remediation_timeline(vulnerability)
                })
        else:
            # Network-wide remediation
            # Group by vulnerability type
            vuln_groups = defaultdict(list)
            for vulnerability in vulnerabilities:
                vuln_groups[vulnerability['type']].append(vulnerability.get('node_id'))
            
            # Generate steps for each vulnerability type
            for vuln_type, affected_nodes in vuln_groups.items():
                pattern = self.vulnerability_patterns.get(vuln_type, {})
                
                remediation_steps.append({
                    'vulnerability': vuln_type,
                    'description': pattern.get('description', 'Unknown vulnerability'),
                    'affected_nodes': affected_nodes,
                    'steps': pattern.get('recommended_actions', []),
                    'priority': 'High' if len(affected_nodes) > 2 else 'Medium'
                })
        
        # Sort remediation steps by severity/priority
        if plan_type == 'node':
            remediation_steps.sort(key=lambda x: x['severity'], reverse=True)
        else:
            priority_order = {'High': 0, 'Medium': 1, 'Low': 2}
            remediation_steps.sort(key=lambda x: priority_order.get(x['priority'], 3))
        
        # Build the remediation plan
        plan = {
            'timestamp': time.time(),
            'plan_type': plan_type,
            'target': node_id if plan_type == 'node' else 'network',
            'overview': self._generate_plan_overview(remediation_steps, plan_type),
            'remediation_steps': remediation_steps,
            'estimated_effort': self._estimate_remediation_effort(remediation_steps),
            'implementation_order': self._suggest_implementation_order(remediation_steps)
        }
        
        return plan
    
    # Helper methods
    def _calculate_base_risk(self, node_data: Dict) -> float:
        """Calculate base risk score for a node"""
        # Get security score (if available) or default to 0.5
        security_score = node_data.get('security_score', 0.5)
        
        # Invert so higher score means higher risk (1.0 - security_score)
        risk_score = 1.0 - security_score
        
        # Adjust for node type
        node_type = node_data.get('type', 'unknown').lower()
        type_risk = self.node_type_risks.get(node_type, 0.5)
        
        # Adjust for protocols used
        protocol_risk = 0.0
        protocols = node_data.get('protocols', [])
        if protocols:
            for protocol in protocols:
                protocol_risk += self.protocol_risks.get(protocol, 0.5)
            protocol_risk /= len(protocols)
        else:
            protocol_risk = 0.5  # Default if no protocols specified
        
        # Calculate weighted base risk
        base_risk = (risk_score * 0.6) + (type_risk * 0.3) + (protocol_risk * 0.1)
        
        return base_risk
    
    def _calculate_event_risk(self, security_events: List[Dict]) -> float:
        """Calculate risk based on recent security events"""
        if not security_events:
            return 0.0
        
        # Calculate risk based on event severity and recency
        event_risk = 0.0
        recent_time = time.time() - (24 * 3600)  # Last 24 hours
        
        for event in security_events:
            severity = event.get('severity', 0.5)
            timestamp = event.get('timestamp', 0)
            
            # Apply recency factor (more recent events have higher weight)
            recency_factor = 1.0
            if timestamp >= recent_time:
                recency_factor = 1.5  # 50% boost for very recent events
            
            event_risk += severity * recency_factor
        
        # Normalize based on number of events
        event_risk = min(event_risk / (len(security_events) * 1.5), 1.0)
        
        return event_risk
    
    def _calculate_network_position_risk(self, node_id: int, node_data: Dict, network_stats: Dict) -> float:
        """Calculate risk based on node's position in the network"""
        # Get node connections
        connections = node_data.get('connections', [])
        connection_count = len(connections)
        
        # Get average connections in the network
        avg_connections = network_stats.get('avg_connections', 1)
        
        # Calculate connectivity risk factor
        # Nodes with more connections than average are more critical
        if avg_connections > 0:
            connectivity_factor = min(connection_count / avg_connections, 3.0) / 3.0
        else:
            connectivity_factor = 0.5
        
        # Check if node is a gateway or border node
        is_gateway = node_data.get('is_gateway', False)
        if is_gateway:
            gateway_factor = 0.8
        else:
            gateway_factor = 0.3
        
        # Calculate position risk
        position_risk = (connectivity_factor * 0.7) + (gateway_factor * 0.3)
        
        return position_risk
    
    def _identify_potential_vulnerabilities(self, node_data: Dict, security_events: List[Dict]) -> List[Dict]:
        """Identify potential vulnerabilities for a node"""
        vulnerabilities = []
        
        # Get node characteristics
        node_type = node_data.get('type', 'unknown').lower()
        security_score = node_data.get('security_score', 0.5)
        protocols = node_data.get('protocols', [])
        
        # Check each vulnerability pattern
        for vuln_id, pattern in self.vulnerability_patterns.items():
            applies_to = pattern['applies_to']
            
            # Check if vulnerability applies to this node type
            if 'all' in applies_to or node_type in applies_to:
                # Calculate likelihood based on indicators
                likelihood = self._calculate_vulnerability_likelihood(
                    pattern, node_data, security_events
                )
                
                if likelihood > 0.3:  # Only include reasonably likely vulnerabilities
                    # Calculate severity
                    min_cvss, max_cvss = pattern['cvss_range']
                    severity = min_cvss + ((max_cvss - min_cvss) * likelihood)
                    
                    vulnerabilities.append({
                        'type': vuln_id,
                        'description': pattern['description'],
                        'likelihood': likelihood,
                        'severity': severity / 10.0,  # Normalize to 0-1 scale
                        'cvss_score': severity,
                        'indicators': self._match_vulnerability_indicators(pattern, node_data, security_events)
                    })
        
        # Sort by severity (highest first)
        vulnerabilities.sort(key=lambda x: x['severity'], reverse=True)
        
        return vulnerabilities
    
    def _calculate_vulnerability_likelihood(self, pattern: Dict, node_data: Dict, 
                                          security_events: List[Dict]) -> float:
        """Calculate likelihood of a vulnerability being present"""
        # Start with base likelihood
        likelihood = 0.5
        
        # Check for indicators in node data and security events
        indicator_count = 0
        matched_indicators = 0
        
        for indicator in pattern['indicators']:
            indicator_count += 1
            
            # Check node data for indicator
            if (('unencrypted' in indicator.lower() and 
                 any(p in ['HTTP', 'FTP', 'TELNET'] for p in node_data.get('protocols', []))) or
                ('default credentials' in indicator.lower() and node_data.get('default_creds', False)) or
                ('outdated' in indicator.lower() and node_data.get('firmware_age', 0) > 365) or
                ('exposed' in indicator.lower() and len(node_data.get('open_ports', [])) > 5)):
                matched_indicators += 1
            
            # Check security events for indicator
            for event in security_events:
                details = event.get('details', '').lower()
                event_type = event.get('event_type', '').lower()
                
                if (indicator.lower() in details or 
                    indicator.lower() in event_type):
                    matched_indicators += 1
                    break
        
        # Calculate match ratio
        if indicator_count > 0:
            match_ratio = matched_indicators / indicator_count
            # Adjust likelihood based on match ratio
            likelihood = 0.3 + (match_ratio * 0.7)
        
        # Adjust based on security score
        security_score = node_data.get('security_score', 0.5)
        likelihood *= (1.5 - security_score)  # Lower security score increases likelihood
        
        # Normalize to 0-1 range
        likelihood = min(max(likelihood, 0.0), 1.0)
        
        return likelihood
    
    def _match_vulnerability_indicators(self, pattern: Dict, node_data: Dict, 
                                      security_events: List[Dict]) -> List[str]:
        """Match vulnerability indicators found in node data and security events"""
        matched_indicators = []
        
        for indicator in pattern['indicators']:
            # Check node data for indicator
            if (('unencrypted' in indicator.lower() and 
                 any(p in ['HTTP', 'FTP', 'TELNET'] for p in node_data.get('protocols', []))) or
                ('default credentials' in indicator.lower() and node_data.get('default_creds', False)) or
                ('outdated' in indicator.lower() and node_data.get('firmware_age', 0) > 365) or
                ('exposed' in indicator.lower() and len(node_data.get('open_ports', [])) > 5)):
                matched_indicators.append(indicator)
            
            # Check security events for indicator
            for event in security_events:
                details = event.get('details', '').lower()
                event_type = event.get('event_type', '').lower()
                
                if (indicator.lower() in details or 
                    indicator.lower() in event_type):
                    matched_indicators.append(f"{indicator} (from security event)")
                    break
        
        return matched_indicators
    
    def _determine_risk_level(self, risk_score: float) -> str:
        """Determine risk level category from risk score"""
        if risk_score >= 0.8:
            return "Critical"
        elif risk_score >= 0.6:
            return "High"
        elif risk_score >= 0.4:
            return "Medium"
        elif risk_score >= 0.2:
            return "Low"
        else:
            return "Minimal"
    
    def _generate_time_predictions(self, node_data: Dict, overall_risk: float, 
                                 vulnerabilities: List[Dict]) -> Dict:
        """Generate time-based vulnerability predictions"""
        predictions = {}
        
        # Generate predictions for each time horizon
        for horizon in self.time_horizons:
            horizon_name = horizon['name']
            days = horizon['days']
            
            # Calculate prediction factors
            # Time factor: risk increases over time as new vulnerabilities emerge
            time_factor = math.log1p(days) / 10.0
            
            # Current vulnerabilities factor
            vuln_risk = sum(v['severity'] for v in vulnerabilities) / max(len(vulnerabilities), 1)
            
            # Calculate predicted risk for this time horizon
            predicted_risk = min(overall_risk + (time_factor * overall_risk), 1.0)
            
            # Adjust for current vulnerabilities
            predicted_risk = (predicted_risk * 0.7) + (vuln_risk * 0.3)
            
            # Determine predicted vulnerabilities for this time horizon
            predicted_vulnerabilities = []
            for vulnerability in vulnerabilities:
                # Current vulnerabilities persist in predictions
                predicted_vulnerabilities.append(vulnerability['type'])
            
            # Add new potential vulnerabilities that might emerge
            node_type = node_data.get('type', 'unknown').lower()
            if predicted_risk > 0.5:
                for vuln_id, pattern in self.vulnerability_patterns.items():
                    if (vuln_id not in predicted_vulnerabilities and 
                        (node_type in pattern['applies_to'] or 'all' in pattern['applies_to'])):
                        emergence_chance = predicted_risk * 0.5
                        if emergence_chance > 0.3:
                            predicted_vulnerabilities.append(vuln_id)
            
            # Add prediction for this time horizon
            predictions[horizon_name] = {
                'days_ahead': days,
                'predicted_risk': predicted_risk,
                'risk_level': self._determine_risk_level(predicted_risk),
                'potential_vulnerabilities': predicted_vulnerabilities
            }
        
        return predictions
    
    def _generate_recommendations(self, vulnerabilities: List[Dict]) -> List[Dict]:
        """Generate recommendations based on predicted vulnerabilities"""
        recommendations = []
        
        # Process each vulnerability
        for vulnerability in vulnerabilities:
            vuln_type = vulnerability['type']
            pattern = self.vulnerability_patterns.get(vuln_type, {})
            
            # Add recommendation based on vulnerability pattern
            if 'recommended_actions' in pattern:
                recommendations.append({
                    'vulnerability': vuln_type,
                    'description': pattern['description'],
                    'severity': vulnerability['severity'],
                    'actions': pattern['recommended_actions'],
                    'priority': 'High' if vulnerability['severity'] > 0.7 else 
                               'Medium' if vulnerability['severity'] > 0.4 else 'Low'
                })
        
        # Sort by severity (highest first)
        recommendations.sort(key=lambda x: x['severity'], reverse=True)
        
        return recommendations
    
    def _calculate_network_stats(self, nodes: Dict) -> Dict:
        """Calculate network-wide statistics"""
        if not nodes:
            return {
                'node_count': 0,
                'avg_connections': 0,
                'max_connections': 0,
                'gateway_count': 0,
                'avg_security_score': 0.5
            }
        
        # Node count
        node_count = len(nodes)
        
        # Count connections and gateways
        connection_counts = []
        security_scores = []
        gateway_count = 0
        
        for node_id, node_data in nodes.items():
            # Count connections
            connections = node_data.get('connections', [])
            connection_counts.append(len(connections))
            
            # Check if gateway
            if node_data.get('is_gateway', False):
                gateway_count += 1
            
            # Get security score
            security_scores.append(node_data.get('security_score', 0.5))
        
        # Calculate averages
        avg_connections = sum(connection_counts) / node_count if node_count > 0 else 0
        max_connections = max(connection_counts) if connection_counts else 0
        avg_security_score = sum(security_scores) / node_count if node_count > 0 else 0.5
        
        return {
            'node_count': node_count,
            'avg_connections': avg_connections,
            'max_connections': max_connections,
            'gateway_count': gateway_count,
            'avg_security_score': avg_security_score
        }
    
    def _identify_critical_paths(self, nodes: Dict, vulnerable_nodes: List[Dict]) -> List[Dict]:
        """Identify critical paths through the network"""
        critical_paths = []
        
        # Get high risk nodes
        high_risk_nodes = [node['node_id'] for node in vulnerable_nodes 
                          if node['risk_level'] in ['High', 'Critical']]
        
        # Find paths through high risk nodes
        if len(high_risk_nodes) >= 2:
            # Simplified path finding for now
            # In a real implementation, this would use graph algorithms
            for i in range(len(high_risk_nodes) - 1):
                source = high_risk_nodes[i]
                target = high_risk_nodes[i + 1]
                
                # Check if there's a direct connection
                source_connections = nodes.get(str(source), {}).get('connections', [])
                if target in source_connections or str(target) in source_connections:
                    critical_paths.append({
                        'source': source,
                        'target': target,
                        'path': [source, target],
                        'risk_level': 'High'
                    })
        
        # Find paths through gateways
        gateways = [node_id for node_id, node_data in nodes.items() 
                  if node_data.get('is_gateway', False)]
        
        if gateways and high_risk_nodes:
            for gateway in gateways:
                for node in high_risk_nodes:
                    if gateway != node and gateway != str(node):
                        critical_paths.append({
                            'source': gateway,
                            'target': node,
                            'path': [gateway, node],
                            'risk_level': 'Critical',
                            'is_gateway_path': True
                        })
        
        return critical_paths
    
    def _identify_attack_vectors(self, nodes: Dict, vulnerable_nodes: List[Dict], 
                               security_events: List[Dict]) -> List[Dict]:
        """Identify potential attack vectors based on network topology"""
        attack_vectors = []
        
        # Get high risk nodes
        high_risk_nodes = [node['node_id'] for node in vulnerable_nodes 
                          if node['risk_level'] in ['High', 'Critical']]
        
        # Get gateway nodes
        gateways = [node_id for node_id, node_data in nodes.items() 
                  if node_data.get('is_gateway', False)]
        
        # Attack vectors through gateways
        for gateway in gateways:
            # Check if gateway is vulnerable
            is_vulnerable = gateway in high_risk_nodes or str(gateway) in high_risk_nodes
            
            attack_vectors.append({
                'entry_point': gateway,
                'vector_type': 'gateway_compromise',
                'likelihood': 'High' if is_vulnerable else 'Medium',
                'potential_targets': high_risk_nodes,
                'impact': 'Critical',
                'description': 'Network entry via vulnerable gateway'
            })
        
        # Attack vectors through exposed services
        for node_id, node_data in nodes.items():
            if node_data.get('exposed_to_internet', False):
                # Check if exposed node is vulnerable
                is_vulnerable = node_id in high_risk_nodes or node_id in [str(n) for n in high_risk_nodes]
                
                attack_vectors.append({
                    'entry_point': node_id,
                    'vector_type': 'exposed_service',
                    'likelihood': 'High' if is_vulnerable else 'Medium',
                    'potential_targets': high_risk_nodes,
                    'impact': 'High',
                    'description': 'Attack via internet-exposed service'
                })
        
        # Attack vectors based on security events
        event_types = {event.get('event_type', '') for event in security_events}
        
        if 'brute_force' in event_types:
            attack_vectors.append({
                'entry_point': 'multiple',
                'vector_type': 'credential_attack',
                'likelihood': 'High',
                'potential_targets': high_risk_nodes,
                'impact': 'High',
                'description': 'Credential compromise via brute force attempts'
            })
        
        if 'malware' in event_types:
            attack_vectors.append({
                'entry_point': 'multiple',
                'vector_type': 'malware_infection',
                'likelihood': 'High',
                'potential_targets': high_risk_nodes,
                'impact': 'Critical',
                'description': 'Malware infection via email or downloads'
            })
        
        return attack_vectors
    
    def _generate_network_predictions(self, avg_risk: float, high_risk_count: int, 
                                    node_count: int, critical_paths: List[Dict], 
                                    attack_vectors: List[Dict]) -> Dict:
        """Generate network-wide predictions"""
        predictions = {}
        
        # Risk factors
        risk_factors = {
            'network_complexity': 0.5,  # Default value
            'attack_surface': min(len(attack_vectors) / 3, 1.0),
            'critical_path_count': min(len(critical_paths) / 2, 1.0),
            'high_risk_node_ratio': min(high_risk_count / max(node_count, 1), 1.0)
        }
        
        # Generate predictions for each time horizon
        for horizon in self.time_horizons:
            horizon_name = horizon['name']
            days = horizon['days']
            
            # Calculate time-based risk increase
            time_factor = math.log1p(days) / 10.0
            
            # Current network risk adjusted by time factor
            base_predicted_risk = min(avg_risk + (time_factor * avg_risk), 1.0)
            
            # Adjust for risk factors
            predicted_risk = base_predicted_risk
            for factor, value in risk_factors.items():
                predicted_risk += value * horizon['weight'] * 0.1
            
            # Normalize to 0-1 range
            predicted_risk = min(max(predicted_risk, 0.0), 1.0)
            
            # Predict attack likelihood
            attack_likelihood = predicted_risk * (1 + len(attack_vectors) / 10)
            attack_likelihood = min(max(attack_likelihood, 0.0), 1.0)
            
            # Add prediction for this time horizon
            predictions[horizon_name] = {
                'days_ahead': days,
                'predicted_risk': predicted_risk,
                'risk_level': self._determine_risk_level(predicted_risk),
                'attack_likelihood': attack_likelihood,
                'potential_attack_vectors': [av['vector_type'] for av in attack_vectors[:3]],
                'vulnerable_segments': self._predict_vulnerable_segments(critical_paths, attack_vectors)
            }
        
        return predictions
    
    def _predict_vulnerable_segments(self, critical_paths: List[Dict], 
                                   attack_vectors: List[Dict]) -> List[Dict]:
        """Predict vulnerable network segments"""
        segments = []
        
        # Create segments from critical paths
        for path in critical_paths:
            segments.append({
                'segment': f"{path['source']}-{path['target']}",
                'risk_level': path['risk_level'],
                'involves_gateway': path.get('is_gateway_path', False)
            })
        
        # Create segments from attack vectors
        for vector in attack_vectors:
            targets = vector.get('potential_targets', [])
            if targets:
                for target in targets[:2]:  # Limit to first 2 targets
                    segments.append({
                        'segment': f"{vector['entry_point']}-{target}",
                        'risk_level': 'High',
                        'attack_vector': vector['vector_type']
                    })
        
        # Deduplicate segments
        unique_segments = []
        segment_names = set()
        
        for segment in segments:
            name = segment['segment']
            if name not in segment_names:
                segment_names.add(name)
                unique_segments.append(segment)
        
        return unique_segments[:5]  # Return top 5 segments
    
    def _generate_network_recommendations(self, vulnerable_nodes: List[Dict], 
                                        critical_paths: List[Dict], 
                                        attack_vectors: List[Dict]) -> List[Dict]:
        """Generate recommendations for improving network security"""
        recommendations = []
        
        # Recommendations based on vulnerable nodes
        high_risk_vulns = defaultdict(int)
        for node in vulnerable_nodes:
            if node['risk_level'] in ['High', 'Critical']:
                for vuln in node.get('predicted_vulnerabilities', []):
                    high_risk_vulns[vuln] += 1
        
        # Add recommendations for common vulnerabilities
        for vuln_type, count in high_risk_vulns.items():
            pattern = self.vulnerability_patterns.get(vuln_type, {})
            if pattern and count >= 2:
                recommendations.append({
                    'type': 'vulnerability_remediation',
                    'vulnerability': vuln_type,
                    'description': f"Address {pattern.get('description', 'vulnerability')} across network",
                    'affected_nodes': count,
                    'actions': pattern.get('recommended_actions', []),
                    'priority': 'High' if count > 3 else 'Medium'
                })
        
        # Recommendations based on critical paths
        if len(critical_paths) >= 2:
            recommendations.append({
                'type': 'network_segmentation',
                'description': 'Implement network segmentation to isolate critical paths',
                'affected_segments': len(critical_paths),
                'actions': [
                    'Deploy firewalls between network segments',
                    'Implement VLANs to isolate critical systems',
                    'Apply access control lists between segments'
                ],
                'priority': 'High'
            })
        
        # Recommendations based on attack vectors
        gateway_vectors = [av for av in attack_vectors if av['vector_type'] == 'gateway_compromise']
        exposed_vectors = [av for av in attack_vectors if av['vector_type'] == 'exposed_service']
        
        if gateway_vectors:
            recommendations.append({
                'type': 'gateway_security',
                'description': 'Enhance security for network gateways',
                'affected_gateways': len(gateway_vectors),
                'actions': [
                    'Implement multi-factor authentication for gateway access',
                    'Deploy intrusion prevention systems',
                    'Regular security audits for gateway devices',
                    'Keep gateway firmware updated'
                ],
                'priority': 'Critical'
            })
        
        if exposed_vectors:
            recommendations.append({
                'type': 'reduce_exposure',
                'description': 'Reduce internet exposure of vulnerable services',
                'affected_services': len(exposed_vectors),
                'actions': [
                    'Implement a VPN for remote access instead of direct exposure',
                    'Use a reverse proxy with WAF for exposed web services',
                    'Close unnecessary ports on internet-facing devices',
                    'Regular vulnerability scanning for exposed services'
                ],
                'priority': 'High'
            })
        
        # Add general recommendations
        recommendations.append({
            'type': 'monitoring_improvement',
            'description': 'Enhance security monitoring',
            'actions': [
                'Deploy a SIEM solution for centralized log analysis',
                'Implement automated alerting for suspicious activity',
                'Deploy network traffic analysis tools',
                'Regular security assessments'
            ],
            'priority': 'Medium'
        })
        
        # Sort by priority
        priority_order = {'Critical': 0, 'High': 1, 'Medium': 2, 'Low': 3}
        recommendations.sort(key=lambda x: priority_order.get(x['priority'], 4))
        
        return recommendations
    
    def _get_node_connections(self, network_data: Dict, node_id: int) -> List[int]:
        """Get connections for a node"""
        nodes = network_data.get('nodes', {})
        
        # Try both string and integer keys
        node = nodes.get(str(node_id)) or nodes.get(node_id)
        
        if node:
            return node.get('connections', [])
        
        return []
    
    def _calculate_propagation(self, network_data: Dict, vulnerable_node_id: int, 
                             connections: List) -> Dict:
        """Calculate vulnerability propagation through the network"""
        nodes = network_data.get('nodes', {})
        
        # Initialize propagation data
        visited = set([vulnerable_node_id])
        paths = []
        affected_nodes = [vulnerable_node_id]
        
        # Queue for BFS with (node, path, probability)
        queue = [(vulnerable_node_id, [vulnerable_node_id], 1.0)]
        
        while queue:
            current, path, probability = queue.pop(0)
            
            # Get connections for current node
            current_connections = self._get_node_connections(network_data, current)
            
            for next_node in current_connections:
                if next_node not in visited:
                    # Calculate propagation probability to this node
                    next_node_str = str(next_node)
                    next_node_data = nodes.get(next_node_str) or nodes.get(next_node, {})
                    security_score = next_node_data.get('security_score', 0.5)
                    
                    # Lower security score means higher propagation probability
                    prop_probability = (1 - security_score) * probability
                    
                    # Only consider significant propagation paths
                    if prop_probability > 0.2:
                        new_path = path + [next_node]
                        paths.append({
                            'path': new_path,
                            'probability': prop_probability,
                            'path_length': len(new_path)
                        })
                        
                        affected_nodes.append(next_node)
                        visited.add(next_node)
                        
                        # Continue propagation if probability is still significant
                        if prop_probability > 0.3 and len(new_path) < 5:
                            queue.append((next_node, new_path, prop_probability))
        
        # Sort paths by probability
        paths.sort(key=lambda x: x['probability'], reverse=True)
        
        # Calculate overall propagation metrics
        overall_probability = max([p['probability'] for p in paths]) if paths else 0
        worst_case_impact = len(affected_nodes) / len(nodes) if nodes else 0
        time_to_propagate = 3600 * (1 + len(paths))  # Simple estimate in seconds
        
        return {
            'paths': paths[:5],  # Top 5 propagation paths
            'affected_nodes': list(set(affected_nodes)),
            'probability': overall_probability,
            'worst_case_impact': worst_case_impact,
            'time_to_propagate': time_to_propagate  # seconds
        }
    
    def _generate_propagation_remediations(self, network_data: Dict, vulnerable_node_id: int, 
                                         propagation_paths: List[Dict]) -> List[Dict]:
        """Generate remediations to prevent vulnerability propagation"""
        remediations = []
        
        # Add remediation for the source node
        remediations.append({
            'type': 'source_remediation',
            'description': 'Fix vulnerability in source node',
            'target': vulnerable_node_id,
            'actions': [
                'Apply security patches',
                'Implement access controls',
                'Deploy host-based security'
            ],
            'priority': 'Critical'
        })
        
        # Add remediations for critical paths
        if propagation_paths:
            critical_nodes = set()
            for path_data in propagation_paths:
                path = path_data.get('path', [])
                if len(path) > 1:
                    # Skip the first node (source)
                    for node in path[1:]:
                        critical_nodes.add(node)
            
            if critical_nodes:
                remediations.append({
                    'type': 'isolate_nodes',
                    'description': 'Isolate critical nodes in propagation paths',
                    'targets': list(critical_nodes),
                    'actions': [
                        'Implement network segmentation',
                        'Apply access control lists',
                        'Deploy intrusion prevention systems'
                    ],
                    'priority': 'High'
                })
        
        # Add general remediations
        remediations.append({
            'type': 'monitoring_enhancement',
            'description': 'Enhance monitoring for propagation indicators',
            'actions': [
                'Deploy behavior-based monitoring',
                'Implement alert correlation',
                'Setup automated containment measures'
            ],
            'priority': 'Medium'
        })
        
        return remediations
    
    def _calculate_severity(self, vulnerability: Dict) -> float:
        """Calculate vulnerability severity"""
        if 'severity' in vulnerability:
            return vulnerability['severity']
        
        # Default severity calculation
        likelihood = vulnerability.get('likelihood', 0.5)
        impact = 0.7  # Default impact
        
        return likelihood * impact
    
    def _suggest_remediation_timeline(self, vulnerability: Dict) -> Dict:
        """Suggest timeline for remediation"""
        severity = self._calculate_severity(vulnerability)
        
        if severity > 0.8:
            return {'urgency': 'Immediate', 'timeframe': '24 hours'}
        elif severity > 0.6:
            return {'urgency': 'High', 'timeframe': '72 hours'}
        elif severity > 0.4:
            return {'urgency': 'Medium', 'timeframe': '1 week'}
        else:
            return {'urgency': 'Low', 'timeframe': '2 weeks'}
    
    def _generate_plan_overview(self, remediation_steps: List[Dict], plan_type: str) -> str:
        """Generate overview text for remediation plan"""
        if not remediation_steps:
            return "No vulnerabilities identified requiring remediation."
        
        high_priority = sum(1 for step in remediation_steps 
                          if step.get('priority') in ['Critical', 'High'])
        
        if plan_type == 'node':
            return (f"Remediation plan addresses {len(remediation_steps)} vulnerabilities, "
                   f"with {high_priority} high-priority items requiring immediate attention.")
        else:
            return (f"Network-wide remediation plan addresses {len(remediation_steps)} issues, "
                   f"with {high_priority} high-priority items requiring immediate attention.")
    
    def _estimate_remediation_effort(self, remediation_steps: List[Dict]) -> Dict:
        """Estimate effort required for remediation"""
        # Simple effort estimation
        total_steps = sum(len(step.get('steps', step.get('actions', []))) 
                         for step in remediation_steps)
        
        # Count high priority items
        high_priority = sum(1 for step in remediation_steps 
                          if step.get('priority') in ['Critical', 'High'])
        
        # Estimate resource requirements
        if total_steps > 15 or high_priority > 3:
            resource_level = 'High'
        elif total_steps > 8 or high_priority > 1:
            resource_level = 'Medium'
        else:
            resource_level = 'Low'
        
        # Estimate timeline
        if high_priority > 3:
            timeline = '1-2 weeks'
        elif total_steps > 12:
            timeline = '2-4 weeks'
        else:
            timeline = '1 week'
        
        return {
            'total_action_items': total_steps,
            'high_priority_items': high_priority,
            'resource_requirement': resource_level,
            'estimated_timeline': timeline
        }
    
    def _suggest_implementation_order(self, remediation_steps: List[Dict]) -> List[Dict]:
        """Suggest order for implementing remediation steps"""
        # Map priorities to numeric values
        priority_values = {
            'Critical': 10,
            'High': 7,
            'Medium': 4,
            'Low': 1
        }
        
        # Calculate implementation score (priority * ease of implementation)
        implementation_order = []
        for i, step in enumerate(remediation_steps):
            priority = step.get('priority', 'Medium')
            priority_value = priority_values.get(priority, 4)
            
            # Estimate ease of implementation (inverse of action count)
            actions = step.get('steps', step.get('actions', []))
            action_count = len(actions)
            ease = 1.0 / max(action_count, 1)
            
            # Calculate score
            score = priority_value * (0.3 + 0.7 * ease)
            
            implementation_order.append({
                'step_index': i,
                'description': step.get('description', ''),
                'priority': priority,
                'score': score
            })
        
        # Sort by score (highest first)
        implementation_order.sort(key=lambda x: x['score'], reverse=True)
        
        # Format result
        result = []
        for i, item in enumerate(implementation_order):
            result.append({
                'order': i + 1,
                'step_index': item['step_index'],
                'description': item['description'],
                'priority': item['priority']
            })
        
        return result